{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from config import config\n",
    "import requests\n",
    "import csv\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_code(code, manual, R1_metadata, R2_metadata):\n",
    "    \"\"\"\n",
    "    Extracts subject IDs from metadata, based on the provided code or manual input.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    code : str\n",
    "        Type of code to use:\n",
    "        - \"id\": Use the \"Subject ID\" from metadata.\n",
    "        - \"id+comment\": Use the \"Subject ID\" concatenated with \"Comments\" from metadata.\n",
    "        - \"manual\": Use the manual code provided.\n",
    "    manual : list or None\n",
    "        A list of two strings, specifying custom codes for Room 1 and Room 2 subjects.\n",
    "        Should be provided if `code` is set to \"manual\". Default is None.\n",
    "    R1_metadata, R2_metadata : pandas.DataFrame\n",
    "        Metadata DataFrames for subjects in Room 1 and Room 2.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    tuple\n",
    "        (code_1, code_2): Codes for subjects in Room 1 and Room 2.\n",
    "\n",
    "    Raises:\n",
    "    ------\n",
    "    ValueError\n",
    "        If `code` parameter is invalid or manual input is incorrect.\n",
    "    \"\"\"\n",
    "    if code == \"id\":\n",
    "        code_1 = R1_metadata[\"Subject ID\"].iloc[0]\n",
    "        code_2 = R2_metadata[\"Subject ID\"].iloc[0]\n",
    "    elif code == \"id+comment\":\n",
    "        code_1 = R1_metadata[\"Subject ID\"].iloc[0] + '_' + R1_metadata[\"Comments\"].iloc[0]\n",
    "        code_2 = R2_metadata[\"Subject ID\"].iloc[0] + '_' + R2_metadata[\"Comments\"].iloc[0]\n",
    "    elif code == \"manual\" or manual != None:\n",
    "        try:\n",
    "            code_1 = manual[0]\n",
    "            code_2 = manual[1]\n",
    "        except ValueError as e:\n",
    "            print(\"You have tried to enter a manual code (this is the filename that the metadata and data will be saved as). Please make sure your manual code is a list, e.g: ['1234_visit1', '5678_visit1'], where the first entry is for subject in room 1 and the second entry for subject in room 2.\")\n",
    "    else:\n",
    "        raise ValueError(\"The value for the code parameter is not valid. Please choose id, id+comment or manual. Default is id.\")\n",
    "    \n",
    "    return code_1, code_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_meta_data(lines, code, manual, save_csv, path_to_save):\n",
    "    \"\"\"\n",
    "    Extracts metadata for two subjects from text lines and optionally saves it as CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    lines : list of str\n",
    "        Text lines containing metadata, with relevant data starting from line 4.\n",
    "    code : str\n",
    "        Method for generating subject IDs (\"id\", \"id+comment\", or \"manual\").\n",
    "    manual : list or None\n",
    "        Custom codes for subjects in Room 1 and Room 2, required if `code` is \"manual\".\n",
    "    save_csv : bool\n",
    "        Whether to save the extracted metadata to CSV files.\n",
    "    path_to_save : str or None\n",
    "        Directory path for saving CSV files. Uses current directory if None.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    tuple\n",
    "        (code_1, code_2, R1_metadata, R2_metadata): Subject codes and metadata DataFrames.\n",
    "    \"\"\"\n",
    "    header_lines = [line.strip().split('\\t') for line in lines[3:7]]\n",
    "\n",
    "    data_R1 = dict(zip(header_lines[0][1:], header_lines[1]))\n",
    "    data_R2 = dict(zip(header_lines[2][1:], header_lines[3]))\n",
    "\n",
    "    R1_metadata = pd.DataFrame([data_R1])\n",
    "    R2_metadata = pd.DataFrame([data_R2])\n",
    "\n",
    "    code_1, code_2 = check_code(code, manual, R1_metadata, R2_metadata)\n",
    "    \n",
    "    if save_csv:\n",
    "        room1_filename = f'{path_to_save}/{code_1}_WRIC_metadata.csv' if path_to_save else f'{code_1}_WRIC_metadata.csv'\n",
    "        room2_filename = f'{path_to_save}/{code_2}_WRIC_metadata.csv' if path_to_save else f'{code_2}_WRIC_metadata.csv'\n",
    "        R1_metadata.to_csv(room1_filename, index=False)\n",
    "        R2_metadata.to_csv(room2_filename, index=False)\n",
    "        \n",
    "    return code_1, code_2, R1_metadata, R2_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(filepath):\n",
    "    \"\"\"\n",
    "    Opens a WRIC .txt file and reads its content.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to the .txt file.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    list of str\n",
    "        Lines read from the file.\n",
    "\n",
    "    Raises:\n",
    "    ------\n",
    "    TypeError\n",
    "        If the file is not a .txt file.\n",
    "    ValueError\n",
    "        If the file does not start with the expected \"OmniCal software\" header.\n",
    "    FileNotFoundError\n",
    "        If the file does not exist at the given filepath.\n",
    "    \"\"\"\n",
    "    if not filepath.lower().endswith('.txt'):\n",
    "        raise TypeError(\"The file must be a .txt file.\")\n",
    "    try:\n",
    "        with open(filepath, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "            if not lines or not lines[0].startswith(\"OmniCal software\"):\n",
    "                raise ValueError(\"The provided file is not the WRIC data file.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(\"The filepath you provided does not lead to a file.\")\n",
    "        \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wric_df(filepath, lines, save_csv, code_1, code_2, path_to_save):\n",
    "    \"\"\"\n",
    "    Creates DataFrames for WRIC data from a file and optionally saves them as CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to the .txt file containing WRIC data.\n",
    "    lines : list of str\n",
    "        Lines read from the file to locate the data start.\n",
    "    save_csv : bool\n",
    "        Whether to save the DataFrames as CSV files.\n",
    "    code_1, code_2 : str\n",
    "        Codes for subjects in Room 1 and Room 2, used for naming the output files.\n",
    "    path_to_save : str or None\n",
    "        Directory path for saving CSV files. Uses current directory if None.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    tuple\n",
    "        (df_room1, df_room2): DataFrames containing data for Room 1 and Room 2.\n",
    "\n",
    "    Raises:\n",
    "    ------\n",
    "    ValueError\n",
    "        If Date or Time columns are inconsistent across rows.\n",
    "    \"\"\"\n",
    "    # find start of data line\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith(\"Room 1 Set 1\"):  # Detect where the actual data starts\n",
    "            data_start_index = i + 1  # First data row starts after this\n",
    "            break\n",
    "    df = pd.read_csv(filepath, sep=\"\\t\", skiprows=data_start_index)\n",
    "    # there are NaN rows after each Room&Set combination that need to be deleted\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "\n",
    "    # define the new column names\n",
    "    # CAREFUL: Maastricht Instruments confused EE kcal and kJ in their original file, so if they ever fix this, the order of kcal and kJ should be reversed (again) here!\n",
    "    columns = [\n",
    "        \"Date\", \"Time\", \"VO2\", \"VCO2\", \"RER\", \"FiO2\", \"FeO2\", \"FiCO2\", \"FeCO2\", \n",
    "        \"Flow\", \"Activity Monitor\", \"Energy Expenditure (kcal/min)\", \"Energy Expenditure (kJ/min)\", \n",
    "        \"Pressure Ambient\", \"Temperature\", \"Relative Humidity\"\n",
    "    ]\n",
    "    new_columns = []\n",
    "    for set_num in ['S1', 'S2']:\n",
    "        for room in ['R1', 'R2']:\n",
    "            for col in columns:\n",
    "                new_columns.append(f\"{room}_{set_num}_{col}\")\n",
    "    df.columns = new_columns\n",
    "\n",
    "    # Check that time and date columns are consistent across rows\n",
    "    date_columns, time_columns = df.filter(like='Date'), df.filter(like='Time')\n",
    "    if not (date_columns.nunique(axis=1).eq(1).all() and time_columns.nunique(axis=1).eq(1).all()):\n",
    "        raise ValueError(\"Date or Time columns do not match in some rows\")\n",
    "\n",
    "\n",
    "    # Combine Date and Time to DateTime and drop all unecessary date/time columns\n",
    "    df_filtered = df.filter(like='Date').iloc[:, 0].to_frame(name=\"Date\").join(df.filter(like='Time').iloc[:, 0].to_frame(name=\"Time\"))\n",
    "    df_filtered['datetime'] = pd.to_datetime(df_filtered['Date'] + ' ' + df_filtered['Time'], format='%m/%d/%y %H:%M:%S')\n",
    "    df_filtered = df_filtered.drop(columns=['Date', 'Time'])\n",
    "    df = df_filtered.join(df.drop(columns=df.filter(like='Date').columns).drop(columns=df.filter(like='Time').columns))\n",
    "    \n",
    "    # Split dataset by room\n",
    "    df_room1 = df.filter(like='R1')\n",
    "    df_room2 = df.filter(like='R2')\n",
    "\n",
    "    if save_csv:\n",
    "        room1_filename = f'{path_to_save}/{code_1}_WRIC_data.csv' if path_to_save else f'{code_1}_WRIC_data.csv'\n",
    "        room2_filename = f'{path_to_save}/{code_2}_WRIC_data.csv' if path_to_save else f'{code_2}_WRIC_data.csv'\n",
    "        df_room1.to_csv(room1_filename, index=False)\n",
    "        df_room2.to_csv(room2_filename, index=False)\n",
    "        \n",
    "    return df_room1, df_room2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_discrepancies(df, threshold=0.05, individual=False):\n",
    "    \"\"\"\n",
    "    Checks for discrepancies between S1 and S2 measurements in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing WRIC data with columns for S1 and S2 measurements.\n",
    "    threshold : float, optional\n",
    "        Threshold percentage for mean relative delta discrepancies. Default is 0.05 (5%).\n",
    "    individual : bool, optional\n",
    "        If True, checks and reports individual row discrepancies beyond the threshold. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    None\n",
    "        Prints out any detected discrepancies between S1 and S2 measurements.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - Filters out environment parameters like \"Pressure Ambient\", \"Temperature\",\n",
    "      \"Relative Humidity\", and \"Activity Monitor\" from the analysis as they are not measured dually.\n",
    "    - Averages S1 and S2 values for mean relative delta calculations.\n",
    "    \"\"\"\n",
    "    env_params = ['Pressure Ambient', 'Temperature', 'Relative Humidity', 'Activity Monitor']\n",
    "    df_filtered = df.loc[:, ~df.columns.str.contains('|'.join(env_params))]\n",
    "    \n",
    "    s1_columns = df_filtered.filter(like='_S1_').columns\n",
    "    s2_columns = df_filtered.filter(like='_S2_').columns\n",
    "    \n",
    "    discrepancies = []\n",
    "    \n",
    "    for s1_col, s2_col in zip(s1_columns, s2_columns):\n",
    "        s1_values = df[s1_col]\n",
    "        s2_values = df[s2_col]\n",
    "        avg_values = (s1_values + s2_values) / 2\n",
    "\n",
    "        # Calculate the mean relative difference\n",
    "        relative_deltas = (s1_values - s2_values) / avg_values\n",
    "        mean_relative_delta = np.mean(relative_deltas)\n",
    "        \n",
    "        discrepancies.append(f\"{s1_col} and {s2_col} have a mean relative delta of {mean_relative_delta:.4f}.\")\n",
    "\n",
    "        # Check if the mean relative delta exceeds the threshold\n",
    "        if np.abs(mean_relative_delta) > (threshold / 100):\n",
    "            discrepancies.append(\n",
    "                f\"{s1_col} and {s2_col} have a mean relative delta of {mean_relative_delta:.4f}, \"\n",
    "                f\"which exceeds the {threshold}% threshold.\"\n",
    "            )\n",
    "        else:\n",
    "            discrepancies.append(\n",
    "                f\"{s1_col} and {s2_col} have a mean relative delta of {mean_relative_delta:.4f}, \"\n",
    "                f\"which is within the {threshold}% threshold.\"\n",
    "            )\n",
    "\n",
    "        # Check individual values for relative discrepancies beyond the threshold\n",
    "        if individual:\n",
    "            for i, (rel_delta) in enumerate(relative_deltas):\n",
    "                if np.abs(rel_delta) > (threshold / 100):\n",
    "                    discrepancies.append( f\"Row {i+1}: {s1_col} and {s2_col} differ by a relative delta of {rel_delta:.4f}.\")\n",
    "    \n",
    "    # Output the discrepancies\n",
    "    if discrepancies:\n",
    "        for discrepancy in discrepancies:\n",
    "            print(discrepancy)\n",
    "    else:\n",
    "        print(\"No discrepancies found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_measurements(df, method='mean'):\n",
    "    \"\"\"\n",
    "    Combines S1 and S2 measurements in the DataFrame using the specified method.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing WRIC data with S1 and S2 measurement columns.\n",
    "    method : str, optional\n",
    "        Method for combining measurements. Options are:\n",
    "        - 'mean': Average of S1 and S2 (default).\n",
    "        - 'median': Median of S1 and S2.\n",
    "        - 's1': Take S1 measurements.\n",
    "        - 's2': Take S2 measurements.\n",
    "        - 'min': Minimum of S1 and S2.\n",
    "        - 'max': Maximum of S1 and S2.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame with combined measurements.\n",
    "    \n",
    "    Raises:\n",
    "    ------\n",
    "    ValueError\n",
    "        If an unsupported combination method is provided.\n",
    "    \"\"\"\n",
    "    s1_columns = df.filter(like='_S1_').columns\n",
    "    s2_columns = df.filter(like='_S2_').columns\n",
    "    \n",
    "    combined = pd.DataFrame()\n",
    "    \n",
    "    for s1_col, s2_col in zip(s1_columns, s2_columns):\n",
    "        if method == 'mean':\n",
    "            combined_values = (df[s1_col] + df[s2_col]) / 2\n",
    "        elif method == 'median':\n",
    "            combined_values = np.median([df[s1_col], df[s2_col]], axis=0)\n",
    "        elif method == 's1':\n",
    "            combined_values = df[s1_col]\n",
    "        elif method == 's2':\n",
    "            combined_values = df[s1_col]\n",
    "        elif method == 'min':\n",
    "            combined_values = np.minimum(df[s1_col], df[s2_col])\n",
    "        elif method == 'max':\n",
    "            combined_values = np.maximum(df[s1_col], df[s2_col])\n",
    "        else:\n",
    "            raise ValueError(f\"Method '{method}' is not supported. Use 'mean', 'median', 's1', 's2', 'min', or 'max'.\")\n",
    "\n",
    "        # Add the combined values to a new DataFrame\n",
    "        column_name = re.sub(r'^.*?_S[12]_', '', s1_col)\n",
    "        combined[column_name] = combined_values\n",
    "        \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_WRIC_file(filepath, code = \"id\", manual = None, save_csv = True, path_to_save = None, combine = True, method = \"mean\"):\n",
    "    \"\"\"\n",
    "    Preprocesses a WRIC data file, extracting metadata, creating DataFrames, and optionally saving results.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to the WRIC .txt file.\n",
    "    code : str, optional\n",
    "        Method for generating subject IDs (\"id\", \"id+comment\", or \"manual\"). Default is \"id\".\n",
    "    manual : list or None, optional\n",
    "        Custom codes for subjects in Room 1 and Room 2 if `code` is \"manual\". Default is None.\n",
    "    save_csv : bool, optional\n",
    "        Whether to save extracted metadata and data to CSV files. Default is True.\n",
    "    path_to_save : str or None, optional\n",
    "        Directory path for saving CSV files. Uses current directory if None. Default is None.\n",
    "    combine : bool, optional\n",
    "        Whether to combine S1 and S2 measurements. Default is True.\n",
    "    method: str, optional\n",
    "        Method for combining measurements. Options are:\n",
    "        - 'mean': Average of S1 and S2 (default).\n",
    "        - 'median': Median of S1 and S2.\n",
    "        - 's1': Take S1 measurements.\n",
    "        - 's2': Take S2 measurements.\n",
    "        - 'min': Minimum of S1 and S2.\n",
    "        - 'max': Maximum of S1 and S2.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    tuple\n",
    "        (R1_metadata, R2_metadata, df_room1, df_room2):\n",
    "        - Metadata DataFrames for Room 1 and Room 2.\n",
    "        - DataFrames with combined or separate measurements for each room (depending on parameter 'combine')\n",
    "    \"\"\"     \n",
    "    lines = open_file(filepath)\n",
    "    code_1, code_2, R1_metadata, R2_metadata = extract_meta_data(lines, code, manual, save_csv, path_to_save)\n",
    "    df_room1, df_room2 = create_wric_df(filepath, lines, save_csv, code_1, code_2, path_to_save)\n",
    "    if combine:\n",
    "        df_room1 = combine_measurements(df_room1, method)\n",
    "        df_room2 = combine_measurements(df_room2, method)\n",
    "        \n",
    "    if save_csv:\n",
    "        room1_filename = f'{path_to_save}/{code_1}_WRIC_data_combined.csv' if path_to_save else f'{code_1}_WRIC_data_combined.csv'\n",
    "        room2_filename = f'{path_to_save}/{code_2}_WRIC_data_combined.csv' if path_to_save else f'{code_2}_WRIC_data_combined.csv'\n",
    "        df_room1.to_csv(room1_filename, index=False)\n",
    "        df_room2.to_csv(room2_filename, index=False)\n",
    "    \n",
    "    return R1_metadata, R2_metadata, df_room1, df_room2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_file_from_redcap(record_id, fieldname, path = None):\n",
    "    \"\"\"\n",
    "    Exports a file from REDCap based on the specified record ID and field name.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    record_id : str\n",
    "        The unique identifier for the record in REDCap.\n",
    "    fieldname : str\n",
    "        The field name from which to export the file.\n",
    "    path : str or None, optional\n",
    "        The file path where the exported file will be saved. If None, defaults to './tmp/export.raw.txt'.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The function prints the HTTP status code of the export request.\n",
    "    - If a path is not provided, the exported file will be saved in a temporary location,\n",
    "      which will be overwritten if the function is called again.\n",
    "    \"\"\"\n",
    "    fields = {\n",
    "            'token': config['api_token'],\n",
    "            'content': 'file',\n",
    "            'action': 'export',\n",
    "            'record': record_id,\n",
    "            'field': fieldname,\n",
    "        }\n",
    "\n",
    "    r = requests.post(config['api_url'], data=fields)\n",
    "    print('HTTP Status: ' + str(r.status_code))\n",
    "\n",
    "    # This is not intended as downloading and storing the data, but only a temporary saving spot for further processing.\n",
    "    # If you do not want the data to be overwritten, please specify a path yourself.\n",
    "    filepath = path if path else './tmp/export.raw.txt'\n",
    "    f = open(filepath, 'wb')\n",
    "    f.write(r.content)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_to_redcap(filepath, record_id, fieldname):\n",
    "    \"\"\"\n",
    "    Uploads a file to REDCap for a specified record ID and field name.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    filepath : str\n",
    "        The path to the file to be uploaded.\n",
    "    record_id : str\n",
    "        The unique identifier for the record in REDCap.\n",
    "    fieldname : str\n",
    "        The field name to which the file will be uploaded.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The function prints the HTTP status code of the upload request.\n",
    "    \"\"\"\n",
    "\n",
    "    fields = {\n",
    "        'token': config['api_token'],\n",
    "        'content': 'file',\n",
    "        'action': 'import',\n",
    "        'record': record_id,\n",
    "        'field': fieldname,\n",
    "        'returnFormat': 'json'\n",
    "    }\n",
    "\n",
    "    file_obj = open(filepath, 'rb')\n",
    "    r = requests.post(config['api_url'],data=fields,files={'file':file_obj})\n",
    "    file_obj.close()\n",
    "\n",
    "    print('HTTP Status: ' + str(r.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_WRIC_files(csv_file, fieldname, code = \"id\", manual = None, save_csv = True, path_to_save = None, combine = True, method = \"mean\"):\n",
    "    \"\"\"\n",
    "    Iterates through records based on record IDs in a CSV file, exporting and processing WRIC data from REDCap.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    csv_file : str\n",
    "        Path to the CSV file containing record IDs.\n",
    "    fieldname : str\n",
    "        The field name from which to export the WRIC data.\n",
    "    code : str, optional\n",
    "        Method for generating subject IDs (\"id\", \"id+comment\", or \"manual\"). Default is \"id\".\n",
    "    manual : list or None, optional\n",
    "        Custom codes for subjects in Room 1 and Room 2 if `code` is \"manual\". Default is None.\n",
    "    save_csv : bool, optional\n",
    "        Whether to save extracted metadata and data to CSV files. Default is True.\n",
    "    path_to_save : str or None, optional\n",
    "        Directory path for saving CSV files. Uses current directory if None. Default is None.\n",
    "    combine : bool, optional\n",
    "        Whether to combine S1 and S2 measurements into a single DataFrame. Default is True.\n",
    "    method: str, optional\n",
    "        Method for combining measurements. Options are:\n",
    "        - 'mean': Average of S1 and S2 (default).\n",
    "        - 'median': Median of S1 and S2.\n",
    "        - 's1': Take S1 measurements.\n",
    "        - 's2': Take S2 measurements.\n",
    "        - 'min': Minimum of S1 and S2.\n",
    "        - 'max': Maximum of S1 and S2.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where each key is a record ID and each value is a tuple containing:\n",
    "        (R1_metadata, R2_metadata, df_room1, df_room2) for each record.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - Requires a valid API access token configured in `config['api_token']` to interact with REDCap (see ReadMe)\n",
    "    - Ensure the CSV file contains valid record IDs in the first column.\n",
    "    \"\"\"\n",
    "\n",
    "    record_ids = []\n",
    "    with open(csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # Assuming the record IDs are in the first column\n",
    "            record_ids.append(str(row[0])) \n",
    "\n",
    "    dataframes = dict()\n",
    "\n",
    "    for record_id in record_ids:\n",
    "\n",
    "        export_file_from_redcap(record_id, fieldname, path = None)\n",
    "        R1_metadata, R2_metadata, df_room1, df_room2 = preprocess_WRIC_file('./tmp/export.raw.txt', code, manual, save_csv, path_to_save, combine, method)\n",
    "        dataframes[record_id] = (R1_metadata, R2_metadata, df_room1, df_room2)\n",
    "    \n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R1_metadata, R2_metadata, df_room1, df_room2 = preprocess_WRIC_file(\"C:\\Documents\\WRIC_example_data\\Results_1m_copy_anonymised.txt\", code=\"id+comment\", path_to_save=None) #path_to_save=\"C:\\Documents\\WRIC_example_data\"\n",
    "display(df_room1)\n",
    "\n",
    "# do not include discrepancy check standard and explain that without calibration gases no realistic values (only do for actual person values)\n",
    "#check_discrepancies(df_room2, threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_WRIC_files('id.csv', 'upload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import WRIC_preprocessing as wric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R1_metadata, R2_metadata, df_room1, df_room2 = wric.preprocess_WRIC_file(\"./example_data/data.txt\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_room1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
